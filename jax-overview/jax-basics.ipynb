{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2dbfdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a: [1. 2. 3.]\n",
      "Array b: [4. 5. 6.]\n",
      "Sum of a and b: [5. 7. 9.]\n",
      "Dot product of a and b: 32.0\n",
      "Element-wise multiplication of a and b: [ 4. 10. 18.]\n",
      "Sine of a: [0.84147096 0.9092974  0.14112   ]\n",
      "Exponential of b: [ 54.598152 148.41316  403.4288  ]\n",
      "Mean of a: 2.0\n",
      "Standard deviation of b: 0.8164966\n",
      "Reshaped a to (3,1): [[1.]\n",
      " [2.]\n",
      " [3.]]\n",
      "Transpose of b reshaped to (3,1): [[4. 5. 6.]]\n",
      "Stacked arrays a and b vertically:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Stacked arrays a and b horizontally:\n",
      " [1. 2. 3. 4. 5. 6.]\n",
      "Concatenated arrays a and b: [1. 2. 3. 4. 5. 6.]\n",
      "Maximum value in a: 3.0\n",
      "Minimum value in b: 4.0\n",
      "Sum of all elements in a: 6.0\n",
      "Cumulative sum of a: [1. 3. 6.]\n",
      "Unique elements in b: [4. 5. 6.]\n",
      "Sorted a: [1. 2. 3.]\n",
      "Where a > 2: (Array([2], dtype=int32),)\n",
      "Convert JAX array a to NumPy array: [1. 2. 3.]\n",
      "Convert NumPy array back to JAX array: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# J - JIT compilation - Just In Time\n",
    "# A - Automatic differentiation\n",
    "# X - XLA (Accelerated linear algebra)\n",
    "\n",
    "# JAX as NumPy\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.array([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"Array a:\", a)\n",
    "print(\"Array b:\", b)\n",
    "print(\"Sum of a and b:\", a + b)\n",
    "print(\"Dot product of a and b:\", jnp.dot(a, b))\n",
    "print(\"Element-wise multiplication of a and b:\", a * b)\n",
    "print(\"Sine of a:\", jnp.sin(a))\n",
    "print(\"Exponential of b:\", jnp.exp(b))\n",
    "print(\"Mean of a:\", jnp.mean(a))\n",
    "print(\"Standard deviation of b:\", jnp.std(b))\n",
    "print(\"Reshaped a to (3,1):\", a.reshape((3, 1)))\n",
    "print(\"Transpose of b reshaped to (3,1):\", b.reshape((3, 1)).T)\n",
    "print(\"Stacked arrays a and b vertically:\\n\", jnp.vstack((a, b)))\n",
    "print(\"Stacked arrays a and b horizontally:\\n\", jnp.hstack((a, b)))\n",
    "print(\"Concatenated arrays a and b:\", jnp.concatenate((a, b)))\n",
    "print(\"Maximum value in a:\", jnp.max(a))\n",
    "print(\"Minimum value in b:\", jnp.min(b))    \n",
    "print(\"Sum of all elements in a:\", jnp.sum(a))\n",
    "print(\"Cumulative sum of a:\", jnp.cumsum(a))\n",
    "print(\"Unique elements in b:\", jnp.unique(b))\n",
    "print(\"Sorted a:\", jnp.sort(a))\n",
    "print(\"Where a > 2:\", jnp.where(a > 2))\n",
    "import numpy as np\n",
    "print(\"Convert JAX array a to NumPy array:\", np.array(a))   \n",
    "print(\"Convert NumPy array back to JAX array:\", jnp.array(np.array(a)))\n",
    "# This script demonstrates basic usage of JAX as a NumPy replacement.\n",
    "# It covers array creation, arithmetic operations, mathematical functions,\n",
    "# statistical functions, reshaping, stacking, concatenation, and conversion\n",
    "# between JAX arrays and NumPy arrays.\n",
    "# JAX is designed for high-performance numerical computing and can\n",
    "# leverage GPU/TPU acceleration, automatic differentiation, and JIT compilation.\n",
    "# It provides a NumPy-like API for ease of use.\n",
    "# JAX arrays are immutable, meaning that operations on them return new arrays\n",
    "# rather than modifying the original arrays in place.\n",
    "# Example: a[1] = 10.0  (This will raise an error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126adb9",
   "metadata": {},
   "source": [
    "# JIT Compilation in JAX - Deep Dive\n",
    "\n",
    "## What is JIT Compilation?\n",
    "\n",
    "**JIT (Just-In-Time) compilation** is JAX's way of optimizing your code for maximum performance. When you decorate a function with `@jax.jit`, JAX:\n",
    "1. **Traces** your function with abstract values to understand its structure\n",
    "2. **Compiles** it to highly optimized machine code using XLA (Accelerated Linear Algebra)\n",
    "3. **Caches** the compiled version for reuse\n",
    "4. **Executes** the optimized code on subsequent calls\n",
    "\n",
    "Think of it like this: regular Python executes line-by-line (interpreted), while JIT-compiled code is translated into super-fast machine instructions that run directly on your hardware (CPU/GPU/TPU).\n",
    "\n",
    "## Why Use JIT?\n",
    "\n",
    "- **Speed**: 10x-100x faster for numerical computations\n",
    "- **Hardware acceleration**: Automatically leverages GPUs/TPUs\n",
    "- **Optimization**: XLA fuses operations and eliminates redundant computations\n",
    "- **Parallelization**: Automatically parallelizes independent operations\n",
    "\n",
    "## When JIT Compilation FAILS or Behaves Unexpectedly\n",
    "\n",
    "### ‚ö†Ô∏è 1. DATA-DEPENDENT CONTROL FLOW (The Key Issue!)\n",
    "\n",
    "**This is what your tutorial was referring to!** You CANNOT use regular Python `if/else` statements that depend on **array values**:\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS WILL FAIL OR BEHAVE INCORRECTLY:\n",
    "@jax.jit\n",
    "def bad_function(x):\n",
    "    if x > 0:  # ‚ùå Control flow depends on the VALUE of x\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "```\n",
    "\n",
    "**Why?** During tracing, JAX doesn't know the actual value of `x` - it only knows its shape and type. So it can't decide which branch to take!\n",
    "\n",
    "**Solution:** Use JAX's special control flow operations:\n",
    "- `jnp.where(condition, true_val, false_val)` - for element-wise conditionals\n",
    "- `jax.lax.cond(pred, true_fun, false_fun, operand)` - for scalar conditionals\n",
    "- `jax.lax.switch()` - for multiple branches\n",
    "- `jax.lax.select()` - for choosing between values\n",
    "\n",
    "```python\n",
    "# ‚úÖ THIS WORKS:\n",
    "@jax.jit\n",
    "def good_function(x):\n",
    "    return jnp.where(x > 0, x * 2, x * 3)  # ‚úÖ JAX-compatible conditional\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è 2. PYTHON SIDE EFFECTS\n",
    "\n",
    "Side effects are operations that modify state outside the function or interact with the external world:\n",
    "\n",
    "```python\n",
    "# ‚ùå THESE DON'T WORK AS EXPECTED IN JIT:\n",
    "@jax.jit\n",
    "def has_side_effects(x):\n",
    "    print(f\"Value is {x}\")  # ‚ùå Print only happens during tracing!\n",
    "    global counter\n",
    "    counter += 1  # ‚ùå Modifying global state\n",
    "    my_list.append(x)  # ‚ùå Modifying external data structures\n",
    "    return x * 2\n",
    "```\n",
    "\n",
    "**Why?** JIT traces the function ONCE, then caches the compiled version. Side effects only execute during tracing, not during every call!\n",
    "\n",
    "**What happens:**\n",
    "- `print()` statements execute only the first time (during tracing)\n",
    "- Global variables are captured at trace time, not updated during execution\n",
    "- File I/O, database calls, etc. won't work as expected\n",
    "\n",
    "### ‚ö†Ô∏è 3. DATA-DEPENDENT LOOPS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def bad_loop(x):\n",
    "    for i in range(int(x)):  # ‚ùå Loop count depends on x's value\n",
    "        x = x + 1\n",
    "    return x\n",
    "```\n",
    "\n",
    "**Solution:** Use `jax.lax.fori_loop()` or `jax.lax.while_loop()` for dynamic loops.\n",
    "\n",
    "### ‚ö†Ô∏è 4. SHAPE-CHANGING OPERATIONS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def dynamic_shape(x):\n",
    "    if x.sum() > 0:\n",
    "        return x[:10]  # Shape changes based on condition\n",
    "    return x\n",
    "```\n",
    "\n",
    "JIT requires shapes to be known at compile time. Dynamic shapes break this requirement.\n",
    "\n",
    "### ‚ö†Ô∏è 5. IN-PLACE MUTATIONS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def mutate_array(x):\n",
    "    x[0] = 10  # ‚ùå JAX arrays are immutable!\n",
    "    return x\n",
    "```\n",
    "\n",
    "**Solution:** Use `.at[].set()` syntax:\n",
    "```python\n",
    "# ‚úÖ THIS WORKS:\n",
    "@jax.jit\n",
    "def update_array(x):\n",
    "    return x.at[0].set(10)  # ‚úÖ Returns new array\n",
    "```\n",
    "\n",
    "## When to Use JIT\n",
    "\n",
    "‚úÖ **USE JIT FOR:**\n",
    "- Pure functions (no side effects)\n",
    "- Functions operating on JAX arrays\n",
    "- Numerical computations (matrix operations, neural networks, simulations)\n",
    "- Functions called repeatedly with similar input shapes\n",
    "- Performance-critical code\n",
    "\n",
    "‚ùå **DON'T USE JIT FOR:**\n",
    "- Functions with print/debug statements\n",
    "- Code that modifies global state\n",
    "- Functions with Python control flow depending on array values\n",
    "- Small, one-off computations (compilation overhead > benefit)\n",
    "- Code interacting with external systems (files, databases, APIs)\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Keep functions pure**: Input ‚Üí Output, no side effects\n",
    "2. **Use JAX control flow**: `jnp.where()`, `jax.lax.cond()`, etc.\n",
    "3. **Warm up the JIT**: Run once before timing to avoid compilation overhead\n",
    "4. **Use `.block_until_ready()`**: JAX executes asynchronously by default\n",
    "5. **Inspect with `jax.make_jaxpr()`**: See the intermediate representation\n",
    "6. **Static arguments**: Use `static_argnums` for non-array arguments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7894e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up JIT compiler...\n",
      "JIT compilation complete! Compiled code is now cached.\n",
      "\n",
      "Time taken for JIT-compiled Collatz computation: 0.001199 seconds\n",
      "First 10 results: [ 4  1 10  2 16  3 22  4 28  5]\n",
      "Array size: 1,000,000 elements\n",
      "\n",
      "======================================================================\n",
      "JAXPR (JAX's intermediate representation) for the Collatz function:\n",
      "======================================================================\n",
      "let _where = { \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[1000000]\u001b[39m b\u001b[35m:i32[1000000]\u001b[39m c\u001b[35m:i32[1000000]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:i32[1000000]\u001b[39m = select_n a c b\n",
      "  \u001b[34;1min \u001b[39;22m(d,) } in\n",
      "{ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[1000000]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22mf\u001b[35m:i32[1000000]\u001b[39m = pjit[\n",
      "      name=collatz\n",
      "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[1000000]\u001b[39m. \u001b[34;1mlet\n",
      "          \u001b[39;22mg\u001b[35m:i32[1000000]\u001b[39m = pjit[\n",
      "            name=remainder\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[1000000]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] h\n",
      "                j\u001b[35m:bool[]\u001b[39m = eq i 0:i32[]\n",
      "                k\u001b[35m:i32[]\u001b[39m = pjit[\n",
      "                  name=_where\n",
      "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; j\u001b[35m:bool[]\u001b[39m l\u001b[35m:i32[]\u001b[39m i\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                      \u001b[39;22mk\u001b[35m:i32[]\u001b[39m = select_n j i l\n",
      "                    \u001b[34;1min \u001b[39;22m(k,) }\n",
      "                ] j 1:i32[] i\n",
      "                m\u001b[35m:i32[1000000]\u001b[39m = rem e k\n",
      "                n\u001b[35m:bool[1000000]\u001b[39m = ne m 0:i32[]\n",
      "                o\u001b[35m:bool[1000000]\u001b[39m = lt m 0:i32[]\n",
      "                p\u001b[35m:bool[]\u001b[39m = lt k 0:i32[]\n",
      "                q\u001b[35m:bool[1000000]\u001b[39m = ne o p\n",
      "                r\u001b[35m:bool[1000000]\u001b[39m = and q n\n",
      "                s\u001b[35m:i32[1000000]\u001b[39m = add m k\n",
      "                g\u001b[35m:i32[1000000]\u001b[39m = select_n r m s\n",
      "              \u001b[34;1min \u001b[39;22m(g,) }\n",
      "          ] e 2:i32[]\n",
      "          t\u001b[35m:bool[1000000]\u001b[39m = eq g 0:i32[]\n",
      "          u\u001b[35m:i32[1000000]\u001b[39m = pjit[\n",
      "            name=floor_divide\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[1000000]\u001b[39m v\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mw\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] v\n",
      "                x\u001b[35m:i32[1000000]\u001b[39m = div e w\n",
      "                y\u001b[35m:i32[1000000]\u001b[39m = sign e\n",
      "                z\u001b[35m:i32[]\u001b[39m = sign w\n",
      "                ba\u001b[35m:bool[1000000]\u001b[39m = ne y z\n",
      "                bb\u001b[35m:i32[1000000]\u001b[39m = rem e w\n",
      "                bc\u001b[35m:bool[1000000]\u001b[39m = ne bb 0:i32[]\n",
      "                bd\u001b[35m:bool[1000000]\u001b[39m = and ba bc\n",
      "                be\u001b[35m:i32[1000000]\u001b[39m = sub x 1:i32[]\n",
      "                u\u001b[35m:i32[1000000]\u001b[39m = pjit[name=_where jaxpr=_where] bd be x\n",
      "              \u001b[34;1min \u001b[39;22m(u,) }\n",
      "          ] e 2:i32[]\n",
      "          bf\u001b[35m:i32[1000000]\u001b[39m = mul 3:i32[] e\n",
      "          bg\u001b[35m:i32[1000000]\u001b[39m = add bf 1:i32[]\n",
      "          f\u001b[35m:i32[1000000]\u001b[39m = pjit[name=_where jaxpr=_where] t u bg\n",
      "        \u001b[34;1min \u001b[39;22m(f,) }\n",
      "    ] e\n",
      "  \u001b[34;1min \u001b[39;22m(f,) }\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION: Why Python control flow breaks JIT\n",
      "======================================================================\n",
      "Error: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function broken_conditional at C:\\Users\\pndnt\\AppData\\Local\\Temp\\ipykernel_39696\\2741948909.py:90 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\n",
      "This happens because JAX can't determine which branch to take during tracing!\n",
      "\n",
      "‚úÖ Correct version using jnp.where():\n",
      "correct_conditional(5.0) = 10.0\n",
      "correct_conditional(-5.0) = -15.0\n",
      "Both results are correct!\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION: Side effects only happen during tracing\n",
      "======================================================================\n",
      "First call (triggers tracing and compilation):\n",
      "üîç TRACING: Inside function with x = Traced<~float32[]>with<DynamicJaxprTrace>\n",
      "Result: 20.0\n",
      "\n",
      "Second call (uses cached compiled version):\n",
      "Result: 40.0\n",
      "üëÜ Notice: The print inside the function didn't execute!\n",
      "\n",
      "Third call with same shape:\n",
      "Result: 60.0\n",
      "üëÜ Still no print - using cached compilation\n",
      "\n",
      "======================================================================\n",
      "DEMONSTRATION: JIT overhead vs benefit\n",
      "======================================================================\n",
      "Small array (3 elements), 1000 iterations:\n",
      "  Without JIT: 0.006302 seconds\n",
      "  With JIT:    0.003408 seconds\n",
      "  Speedup:     1.85x\n",
      "\n",
      "Large array (1M elements), 100 iterations:\n",
      "  Without JIT: 0.034850 seconds\n",
      "  With JIT:    0.055584 seconds\n",
      "  Speedup:     0.63x\n",
      "\n",
      "‚úÖ Key Takeaway: JIT is beneficial for large computations, not tiny ones!\n",
      "\n",
      "======================================================================\n",
      "KEY TAKEAWAYS\n",
      "======================================================================\n",
      "\n",
      "1. ‚úÖ USE jnp.where() for conditionals, NOT Python if/else with array values\n",
      "2. ‚úÖ Pure functions work best (no side effects like print, global variables)\n",
      "3. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
      "4. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
      "5. ‚úÖ JIT is best for large computations called repeatedly\n",
      "6. ‚úÖ Inspect JAXPR with jax.make_jaxpr() to understand what gets compiled\n",
      "7. ‚ùå Avoid Python control flow that depends on array VALUES\n",
      "8. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
      "9. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# JIT COMPILATION - PRACTICAL EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 1: Basic JIT Compilation with Collatz Conjecture\n",
    "# -----------------------------------------------------------------------------\n",
    "# The Collatz conjecture: Take any positive integer n. If n is even, divide it\n",
    "# by 2. If n is odd, multiply it by 3 and add 1. Repeat the process.\n",
    "# We use jnp.where() instead of if/else because we need JAX-compatible conditionals!\n",
    "\n",
    "@jax.jit\n",
    "def collatz(x):\n",
    "    \"\"\"\n",
    "    Compute one step of the Collatz sequence.\n",
    "    \n",
    "    Args:\n",
    "        x: JAX array of integers\n",
    "    \n",
    "    Returns:\n",
    "        Next value(s) in the Collatz sequence\n",
    "    \n",
    "    Note: Uses jnp.where() for vectorized conditional logic instead of if/else\n",
    "    This allows JIT compilation to work correctly!\n",
    "    \"\"\"\n",
    "    return jnp.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# Create a large array to see the performance benefit\n",
    "arr = jnp.arange(1, 1000001)\n",
    "\n",
    "# IMPORTANT: Warm up the JIT compiler\n",
    "# First call triggers compilation (slow), subsequent calls use cached version (fast)\n",
    "print(\"Warming up JIT compiler...\")\n",
    "_ = collatz(arr).block_until_ready()\n",
    "print(\"JIT compilation complete! Compiled code is now cached.\\n\")\n",
    "\n",
    "# Now measure the actual execution time (excluding compilation)\n",
    "start = time.time()\n",
    "result = collatz(arr).block_until_ready()\n",
    "end = time.time()\n",
    "print(f\"Time taken for JIT-compiled Collatz computation: {end - start:.6f} seconds\")\n",
    "print(f\"First 10 results: {result[:10]}\")\n",
    "print(f\"Array size: {len(result):,} elements\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING ASYNCHRONOUS EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "# JAX executes operations asynchronously by default to maximize performance.\n",
    "# This means JAX queues operations and returns control to Python immediately,\n",
    "# while the actual computation happens in the background on the accelerator.\n",
    "#\n",
    "# .block_until_ready() forces Python to wait until the computation completes.\n",
    "# Without it, you'd measure queue time, not actual computation time!\n",
    "#\n",
    "# Example without block_until_ready():\n",
    "# result = collatz(arr)  # Returns immediately, computation not done yet!\n",
    "# print(result)  # NOW it blocks to print, but timing would be wrong\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING JAXPR - JAX's Intermediate Representation\n",
    "# -----------------------------------------------------------------------------\n",
    "# JAXPR is like assembly language for JAX - it shows the low-level operations\n",
    "# that XLA will compile and optimize. It's useful for understanding what JIT\n",
    "# actually does with your code.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"JAXPR (JAX's intermediate representation) for the Collatz function:\")\n",
    "print(\"=\" * 70)\n",
    "print(jax.make_jaxpr(collatz)(arr))\n",
    "print()\n",
    "\n",
    "# The JAXPR shows:\n",
    "# - Input parameters and their shapes\n",
    "# - Primitive operations (mod, eq, where, floordiv, mul, add)\n",
    "# - How data flows through the computation\n",
    "# This is what gets sent to XLA for compilation into machine code!\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 2: Demonstrating Why Python if/else Fails\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION: Why Python control flow breaks JIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ‚ùå THIS WILL TRACE INCORRECTLY - only one branch gets compiled!\n",
    "@jax.jit\n",
    "def broken_conditional(x):\n",
    "    \"\"\"\n",
    "    This function will behave incorrectly when JIT-compiled!\n",
    "    During tracing, JAX doesn't know x's value, so it picks ONE branch\n",
    "    (usually the first one it encounters) and always uses that.\n",
    "    \"\"\"\n",
    "    if x > 0:  # This evaluates to an ARRAY comparison, not a simple True/False\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "# Test with different values - you'll see unexpected behavior!\n",
    "try:\n",
    "    # During tracing, JAX treats (x > 0) as a tracer, not a boolean\n",
    "    # This may work but will give wrong results or raise a ConcretizationError\n",
    "    test_val = jnp.array(5.0)\n",
    "    result_pos = broken_conditional(test_val)\n",
    "    print(f\"broken_conditional(5.0) = {result_pos}\")  # Might work once\n",
    "    \n",
    "    test_val = jnp.array(-5.0)\n",
    "    result_neg = broken_conditional(test_val)\n",
    "    print(f\"broken_conditional(-5.0) = {result_neg}\")  # Will give WRONG answer!\n",
    "    print(\"‚ö†Ô∏è  Notice: Both might return the same result because only one branch was traced!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This happens because JAX can't determine which branch to take during tracing!\\n\")\n",
    "\n",
    "# ‚úÖ CORRECT VERSION using jnp.where()\n",
    "@jax.jit\n",
    "def correct_conditional(x):\n",
    "    \"\"\"\n",
    "    Correct version using JAX-compatible control flow.\n",
    "    jnp.where() evaluates BOTH branches and selects based on condition.\n",
    "    This works with JIT because no Python control flow is needed!\n",
    "    \"\"\"\n",
    "    return jnp.where(x > 0, x * 2, x * 3)\n",
    "\n",
    "# Test the correct version\n",
    "print(\"‚úÖ Correct version using jnp.where():\")\n",
    "result_pos = correct_conditional(jnp.array(5.0))\n",
    "result_neg = correct_conditional(jnp.array(-5.0))\n",
    "print(f\"correct_conditional(5.0) = {result_pos}\")\n",
    "print(f\"correct_conditional(-5.0) = {result_neg}\")\n",
    "print(\"Both results are correct!\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 3: Side Effects in JIT - Print Statements\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION: Side effects only happen during tracing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "@jax.jit\n",
    "def function_with_print(x):\n",
    "    \"\"\"\n",
    "    Print statements only execute during the FIRST call (tracing phase).\n",
    "    Subsequent calls use the cached compiled code, which doesn't include prints!\n",
    "    \"\"\"\n",
    "    print(f\"üîç TRACING: Inside function with x = {x}\")\n",
    "    return x * 2\n",
    "\n",
    "print(\"First call (triggers tracing and compilation):\")\n",
    "result1 = function_with_print(jnp.array(10.0))\n",
    "print(f\"Result: {result1}\\n\")\n",
    "\n",
    "print(\"Second call (uses cached compiled version):\")\n",
    "result2 = function_with_print(jnp.array(20.0))\n",
    "print(f\"Result: {result2}\")\n",
    "print(\"üëÜ Notice: The print inside the function didn't execute!\\n\")\n",
    "\n",
    "print(\"Third call with same shape:\")\n",
    "result3 = function_with_print(jnp.array(30.0))\n",
    "print(f\"Result: {result3}\")\n",
    "print(\"üëÜ Still no print - using cached compilation\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 4: When NOT to use JIT (Compilation Overhead)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMONSTRATION: JIT overhead vs benefit\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def simple_add_no_jit(x):\n",
    "    return x + 1\n",
    "\n",
    "@jax.jit\n",
    "def simple_add_with_jit(x):\n",
    "    return x + 1\n",
    "\n",
    "# Small array - JIT overhead dominates\n",
    "small_arr = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_no_jit(small_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "# Warm up JIT\n",
    "_ = simple_add_with_jit(small_arr).block_until_ready()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_with_jit(small_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"Small array (3 elements), 1000 iterations:\")\n",
    "print(f\"  Without JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  With JIT:    {time_with_jit:.6f} seconds\")\n",
    "print(f\"  Speedup:     {time_no_jit/time_with_jit:.2f}x\")\n",
    "print()\n",
    "\n",
    "# Large array - JIT benefit is clear\n",
    "large_arr = jnp.arange(1000000.0)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_no_jit(large_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "# Warm up JIT\n",
    "_ = simple_add_with_jit(large_arr).block_until_ready()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_with_jit(large_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"Large array (1M elements), 100 iterations:\")\n",
    "print(f\"  Without JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  With JIT:    {time_with_jit:.6f} seconds\")\n",
    "print(f\"  Speedup:     {time_no_jit/time_with_jit:.2f}x\")\n",
    "print(\"\\n‚úÖ Key Takeaway: JIT is beneficial for large computations, not tiny ones!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY OF JIT COMPILATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. ‚úÖ USE jnp.where() for conditionals, NOT Python if/else with array values\n",
    "2. ‚úÖ Pure functions work best (no side effects like print, global variables)\n",
    "3. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
    "4. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
    "5. ‚úÖ JIT is best for large computations called repeatedly\n",
    "6. ‚úÖ Inspect JAXPR with jax.make_jaxpr() to understand what gets compiled\n",
    "7. ‚ùå Avoid Python control flow that depends on array VALUES\n",
    "8. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
    "9. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c7b93",
   "metadata": {},
   "source": [
    "## Quick Reference: JIT-Compatible Control Flow\n",
    "\n",
    "When you need conditionals in JIT-compiled functions, use these JAX operations:\n",
    "\n",
    "| Scenario | ‚ùå Don't Use | ‚úÖ Use Instead |\n",
    "|----------|--------------|----------------|\n",
    "| Element-wise conditional | `if x > 0: ...` | `jnp.where(x > 0, true_val, false_val)` |\n",
    "| Scalar conditional | `if x > 0: ...` | `jax.lax.cond(x > 0, true_fn, false_fn, operand)` |\n",
    "| Multiple branches | `if/elif/else` | `jax.lax.switch(index, branches, operand)` |\n",
    "| Dynamic loops | `for i in range(int(x)): ...` | `jax.lax.fori_loop(start, end, body_fn, init)` |\n",
    "| While loops | `while condition: ...` | `jax.lax.while_loop(cond_fn, body_fn, init)` |\n",
    "| Array updates | `arr[i] = val` | `arr.at[i].set(val)` |\n",
    "\n",
    "**Why?** During JIT tracing, JAX works with abstract values (shapes/types), not actual data. Python control flow needs concrete values, which aren't available during tracing. JAX's control flow ops are designed to work with abstract values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1a537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
