{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa023aa7",
   "metadata": {},
   "source": [
    "# üìò Notebook 2: JIT Compilation - Making Your Code Blazing Fast\n",
    "\n",
    "Welcome to the \"speed\" chapter of JAX! This notebook teaches you how to make your code **10-100x faster** using JIT compilation.\n",
    "\n",
    "## üéØ What You'll Learn (30-40 minutes)\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- ‚úÖ What JIT (Just-In-Time) compilation is and why it's powerful\n",
    "- ‚úÖ How to use `@jax.jit` to accelerate code\n",
    "- ‚úÖ **Critical pitfalls** that break JIT (and how to avoid them)\n",
    "- ‚úÖ When JIT helps (and when it doesn't)\n",
    "- ‚úÖ How to debug JIT issues\n",
    "\n",
    "## ü§î What is JIT Compilation?\n",
    "\n",
    "### The Problem: Python is Slow\n",
    "Regular Python code is **interpreted** - executed line by line at runtime. This is flexible but slow for numerical computations.\n",
    "\n",
    "### The Solution: JIT Compilation\n",
    "**JIT (Just-In-Time) compilation** converts your Python function to **optimized machine code** that runs directly on your hardware.\n",
    "\n",
    "### How JIT Works (4 Steps):\n",
    "1. **Trace**: First time you call a JIT function, JAX traces it to understand what it does\n",
    "2. **Compile**: JAX compiles it to optimized machine code using XLA\n",
    "3. **Cache**: The compiled version is cached for reuse\n",
    "4. **Execute**: Future calls use the fast compiled version\n",
    "\n",
    "```python\n",
    "@jax.jit  # This decorator enables JIT compilation\n",
    "def fast_function(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "# First call: Traces, compiles, executes (slow)\n",
    "result = fast_function(5.0)\n",
    "\n",
    "# Second call: Uses cached compiled version (FAST!)\n",
    "result = fast_function(10.0)\n",
    "```\n",
    "\n",
    "### The Speedup\n",
    "- **Small arrays**: 2-5x faster\n",
    "- **Medium arrays**: 10-50x faster\n",
    "- **Large arrays**: 50-100x faster\n",
    "- **Complex operations**: Can be 1000x+ faster!\n",
    "\n",
    "## ‚ö†Ô∏è JIT Pitfalls (Very Important!)\n",
    "\n",
    "JIT is powerful but has **strict requirements**. The #1 source of JAX errors!\n",
    "\n",
    "### Pitfall #1: Data-Dependent Control Flow ‚ùå\n",
    "**This FAILS:**\n",
    "```python\n",
    "@jax.jit\n",
    "def broken(x):\n",
    "    if x > 0:  # ‚ùå Control flow depends on x's VALUE\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "```\n",
    "\n",
    "**Why?** During tracing, JAX doesn't know x's value - only its shape and type!\n",
    "\n",
    "**The Fix:**\n",
    "```python\n",
    "@jax.jit\n",
    "def works(x):\n",
    "    return jnp.where(x > 0, x * 2, x * 3)  # ‚úÖ JAX-compatible\n",
    "```\n",
    "\n",
    "### Pitfall #2: Side Effects Don't Work ‚ùå\n",
    "**This FAILS (or behaves unexpectedly):**\n",
    "```python\n",
    "@jax.jit\n",
    "def broken(x):\n",
    "    print(f\"x = {x}\")  # ‚ùå Only prints ONCE (during tracing)\n",
    "    global counter\n",
    "    counter += 1  # ‚ùå Global state isn't updated\n",
    "    return x * 2\n",
    "```\n",
    "\n",
    "**Why?** JIT traces once, then reuses the compiled code. Side effects only happen during tracing!\n",
    "\n",
    "### Pitfall #3: Dynamic Shapes ‚ùå\n",
    "JIT requires shapes to be known at compile time. Dynamic shapes break this.\n",
    "\n",
    "### Pitfall #4: In-Place Mutations ‚ùå\n",
    "Remember from Notebook 1? JAX arrays are immutable. Use `.at[].set()` instead.\n",
    "\n",
    "## üéì What's in This Notebook?\n",
    "\n",
    "1. **Performance comparisons** - JIT vs non-JIT (see the speedup!)\n",
    "2. **Understanding async execution** - Why `.block_until_ready()` matters\n",
    "3. **JAXPR inspection** - Peek under the hood\n",
    "4. **Common pitfalls** - Data-dependent control flow, side effects\n",
    "5. **Debugging JIT issues** - How to fix errors\n",
    "6. **When NOT to use JIT** - Compilation overhead vs benefit\n",
    "7. **Quick reference guide** - JAX-compatible control flow\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before starting this notebook, you should:\n",
    "- ‚úÖ Complete Notebook 1 (JAX Basics)\n",
    "- ‚úÖ Understand what a function is\n",
    "- ‚úÖ Know basic Python control flow (if/else, loops)\n",
    "\n",
    "## üí° Key Takeaway\n",
    "\n",
    "**JIT = Tracing ‚Üí Compiling ‚Üí Caching ‚Üí Fast Execution**\n",
    "\n",
    "The first call is slow (compilation), but subsequent calls are **blazing fast**!\n",
    "\n",
    "Let's see it in action! üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8be0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERFORMANCE COMPARISON: WITH JIT vs WITHOUT JIT\n",
      "======================================================================\n",
      "\n",
      "üîß Warming up JIT compiler (first call triggers compilation)...\n",
      "‚úÖ JIT compilation complete! Compiled code is now cached.\n",
      "\n",
      "\n",
      "üìä SMALL ARRAY - 1,000 elements, 100 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.157027 seconds\n",
      "WITH JIT:            0.057830 seconds\n",
      "SPEEDUP:                 2.72x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "üìä MEDIUM ARRAY - 100,000 elements, 50 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.175127 seconds\n",
      "WITH JIT:            0.037560 seconds\n",
      "SPEEDUP:                 4.66x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "üìä LARGE ARRAY - 1,000,000 elements, 10 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.200553 seconds\n",
      "WITH JIT:            0.013129 seconds\n",
      "SPEEDUP:                15.28x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "======================================================================\n",
      "üìà PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "Small (1K):       2.72x speedup\n",
      "Medium (100K):    4.66x speedup\n",
      "Large (1M):      15.28x speedup\n",
      "\n",
      "‚úÖ Key Insight: JIT speedup increases with array size!\n",
      "\n",
      "======================================================================\n",
      "üîÑ UNDERSTANDING .block_until_ready()\n",
      "======================================================================\n",
      "\n",
      "JAX executes operations ASYNCHRONOUSLY by default for maximum performance.\n",
      "\n",
      "What this means:\n",
      "1. JAX queues operations and returns control to Python immediately\n",
      "2. Actual computation happens in the background on the accelerator\n",
      "3. Without .block_until_ready(), you'd measure queue time, not compute time!\n",
      "\n",
      "Example:\n",
      "    result = collatz(arr)           # Returns instantly, not done yet!\n",
      "    print(result)                   # NOW it blocks to get the value\n",
      "\n",
      "For accurate timing, ALWAYS use .block_until_ready() after the computation:\n",
      "    result = collatz(arr).block_until_ready()  # ‚úÖ Waits for completion\n",
      "\n",
      "======================================================================\n",
      "üîç JAXPR - JAX's Intermediate Representation\n",
      "======================================================================\n",
      "\n",
      "JAXPR is like assembly language for JAX. It shows the low-level operations\n",
      "that XLA compiles into machine code. Think of it as a peek under the hood!\n",
      "\n",
      "\n",
      "JAXPR for collatz_with_jit with input shape (10,):\n",
      "----------------------------------------------------------------------\n",
      "let _where = { \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[10]\u001b[39m b\u001b[35m:i32[10]\u001b[39m c\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:i32[10]\u001b[39m = select_n a c b\n",
      "  \u001b[34;1min \u001b[39;22m(d,) } in\n",
      "{ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22mf\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "      name=collatz_with_jit\n",
      "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "          \u001b[39;22mg\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=remainder\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] h\n",
      "                j\u001b[35m:bool[]\u001b[39m = eq i 0:i32[]\n",
      "                k\u001b[35m:i32[]\u001b[39m = pjit[\n",
      "                  name=_where\n",
      "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; j\u001b[35m:bool[]\u001b[39m l\u001b[35m:i32[]\u001b[39m i\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                      \u001b[39;22mk\u001b[35m:i32[]\u001b[39m = select_n j i l\n",
      "                    \u001b[34;1min \u001b[39;22m(k,) }\n",
      "                ] j 1:i32[] i\n",
      "                m\u001b[35m:i32[10]\u001b[39m = rem e k\n",
      "                n\u001b[35m:bool[10]\u001b[39m = ne m 0:i32[]\n",
      "                o\u001b[35m:bool[10]\u001b[39m = lt m 0:i32[]\n",
      "                p\u001b[35m:bool[]\u001b[39m = lt k 0:i32[]\n",
      "                q\u001b[35m:bool[10]\u001b[39m = ne o p\n",
      "                r\u001b[35m:bool[10]\u001b[39m = and q n\n",
      "                s\u001b[35m:i32[10]\u001b[39m = add m k\n",
      "                g\u001b[35m:i32[10]\u001b[39m = select_n r m s\n",
      "              \u001b[34;1min \u001b[39;22m(g,) }\n",
      "          ] e 2:i32[]\n",
      "          t\u001b[35m:bool[10]\u001b[39m = eq g 0:i32[]\n",
      "          u\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=floor_divide\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m v\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mw\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] v\n",
      "                x\u001b[35m:i32[10]\u001b[39m = div e w\n",
      "                y\u001b[35m:i32[10]\u001b[39m = sign e\n",
      "                z\u001b[35m:i32[]\u001b[39m = sign w\n",
      "                ba\u001b[35m:bool[10]\u001b[39m = ne y z\n",
      "                bb\u001b[35m:i32[10]\u001b[39m = rem e w\n",
      "                bc\u001b[35m:bool[10]\u001b[39m = ne bb 0:i32[]\n",
      "                bd\u001b[35m:bool[10]\u001b[39m = and ba bc\n",
      "                be\u001b[35m:i32[10]\u001b[39m = sub x 1:i32[]\n",
      "                u\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] bd be x\n",
      "              \u001b[34;1min \u001b[39;22m(u,) }\n",
      "          ] e 2:i32[]\n",
      "          bf\u001b[35m:i32[10]\u001b[39m = mul 3:i32[] e\n",
      "          bg\u001b[35m:i32[10]\u001b[39m = add bf 1:i32[]\n",
      "          f\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] t u bg\n",
      "        \u001b[34;1min \u001b[39;22m(f,) }\n",
      "    ] e\n",
      "  \u001b[34;1min \u001b[39;22m(f,) }\n",
      "\n",
      "What you see:\n",
      "  ‚Ä¢ Input parameters (a:i32[10]) - integer array with 10 elements\n",
      "  ‚Ä¢ Primitive operations: mod, eq, where, floordiv, mul, add\n",
      "  ‚Ä¢ Data flow through the computation\n",
      "  ‚Ä¢ This gets sent to XLA for optimization and compilation!\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  DEMONSTRATION: Why Python Control Flow Breaks JIT\n",
      "======================================================================\n",
      "\n",
      "‚ùå Testing broken_conditional with Python if/else:\n",
      "   Error: TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function broken_conditional at C:\\Users\\pndnt\\AppData\\Local\\Temp\\ipykernel_40576\\1548074232.py:145 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\n",
      "   This happens because JAX can't determine the branch during tracing!\n",
      "\n",
      "‚úÖ Testing correct_conditional with jnp.where():\n",
      "   correct_conditional(5.0)  = 10.0  ‚úì\n",
      "   correct_conditional(-5.0) = -15.0 ‚úì\n",
      "   Both results are CORRECT!\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: if/else vs jnp.where()\n",
      "======================================================================\n",
      "Input values:     [ 5. -3.  2. -8.  0.]\n",
      "jnp.where() results: [ 10.  -9.   4. -24.   0.]\n",
      "Expected (x>0 ? 2x : 3x): [10.0, -9.0, 4.0, -24.0, 0.0]\n",
      "Match: True\n",
      "\n",
      "======================================================================\n",
      "üñ®Ô∏è  DEMONSTRATION: Side Effects Only Happen During Tracing\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  First call (triggers tracing and compilation):\n",
      "   üîç TRACING: Inside function with x = Traced<~float32[]>with<DynamicJaxprTrace>\n",
      "   Result: 20.0\n",
      "\n",
      "2Ô∏è‚É£  Second call (uses cached compiled version):\n",
      "   Result: 40.0\n",
      "   üëÜ Notice: The print INSIDE the function didn't execute!\n",
      "\n",
      "3Ô∏è‚É£  Third call (still using cached version):\n",
      "   Result: 60.0\n",
      "   üëÜ Still no print - using cached compilation\n",
      "\n",
      "üí° Key Point: Side effects (print, global vars, I/O) only happen once!\n",
      "\n",
      "======================================================================\n",
      "‚öñÔ∏è  DEMONSTRATION: JIT Overhead vs Benefit\n",
      "======================================================================\n",
      "\n",
      "üìâ TINY ARRAYS (3 elements, 1000 iterations):\n",
      "  WITHOUT JIT: 0.030743 seconds\n",
      "  WITH JIT:    0.005701 seconds\n",
      "  Speedup:     5.39x\n",
      "\n",
      "üìà LARGE ARRAYS (1M elements, 100 iterations):\n",
      "  WITHOUT JIT: 0.068910 seconds\n",
      "  WITH JIT:    0.094652 seconds\n",
      "  Speedup:     0.73x\n",
      "  ‚úÖ JIT provides significant benefit for large computations!\n",
      "\n",
      "======================================================================\n",
      "üéØ KEY TAKEAWAYS - JIT COMPILATION\n",
      "======================================================================\n",
      "\n",
      "1. ‚úÖ JIT provides 2x-100x speedup for large numerical computations\n",
      "2. ‚úÖ Use jnp.where() for conditionals, NOT Python if/else with array values\n",
      "3. ‚úÖ Pure functions work best (no side effects like print, globals, I/O)\n",
      "4. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
      "5. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
      "6. ‚úÖ JIT benefit increases with array size and computation complexity\n",
      "7. ‚úÖ Inspect with jax.make_jaxpr() to see the compiled representation\n",
      "8. ‚ùå Python control flow (if/else) that depends on array VALUES breaks JIT\n",
      "9. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
      "10. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
      "\n",
      "üí° Best Use Cases: Matrix operations, neural networks, scientific simulations,\n",
      "   repeated computations on large arrays\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# JIT COMPILATION - PRACTICAL EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 1: JIT vs Non-JIT Performance Comparison\n",
    "# -----------------------------------------------------------------------------\n",
    "# The Collatz conjecture: Take any positive integer n. If n is even, divide it\n",
    "# by 2. If n is odd, multiply it by 3 and add 1. Repeat the process.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON: WITH JIT vs WITHOUT JIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VERSION 1: WITHOUT JIT\n",
    "def collatz_no_jit(x):\n",
    "    \"\"\"\n",
    "    Collatz step WITHOUT JIT compilation.\n",
    "    Uses jnp.where() for vectorized conditional logic.\n",
    "    \"\"\"\n",
    "    return jnp.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# VERSION 2: WITH JIT\n",
    "@jax.jit\n",
    "def collatz_with_jit(x):\n",
    "    \"\"\"\n",
    "    Collatz step WITH JIT compilation.\n",
    "    Same logic, but decorated with @jax.jit for optimization.\n",
    "    \"\"\"\n",
    "    return jnp.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# Create test arrays of different sizes\n",
    "small_arr = jnp.arange(1, 1001)        # 1K elements\n",
    "medium_arr = jnp.arange(1, 100001)     # 100K elements\n",
    "large_arr = jnp.arange(1, 1000001)     # 1M elements\n",
    "\n",
    "print(\"\\nüîß Warming up JIT compiler (first call triggers compilation)...\")\n",
    "_ = collatz_with_jit(large_arr).block_until_ready()\n",
    "print(\"‚úÖ JIT compilation complete! Compiled code is now cached.\\n\")\n",
    "\n",
    "# Test function for timing\n",
    "def benchmark_comparison(arr, label, iterations=10):\n",
    "    \"\"\"Benchmark both versions and compare results.\"\"\"\n",
    "    print(f\"\\nüìä {label} - {len(arr):,} elements, {iterations} iterations:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Benchmark WITHOUT JIT\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result_no_jit = collatz_no_jit(arr).block_until_ready()\n",
    "    time_no_jit = time.time() - start\n",
    "    \n",
    "    # Benchmark WITH JIT\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result_with_jit = collatz_with_jit(arr).block_until_ready()\n",
    "    time_with_jit = time.time() - start\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = time_no_jit / time_with_jit if time_with_jit > 0 else float('inf')\n",
    "    \n",
    "    # Display results side-by-side\n",
    "    print(f\"{'WITHOUT JIT:':20} {time_no_jit:8.6f} seconds\")\n",
    "    print(f\"{'WITH JIT:':20} {time_with_jit:8.6f} seconds\")\n",
    "    print(f\"{'SPEEDUP:':20} {speedup:8.2f}x faster\")\n",
    "    \n",
    "    # Verify results match\n",
    "    if jnp.allclose(result_no_jit, result_with_jit):\n",
    "        print(f\"‚úÖ Results match! First 5 values: {result_with_jit[:5]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Results differ!\")\n",
    "    \n",
    "    return speedup\n",
    "\n",
    "# Run benchmarks for different array sizes\n",
    "speedup_small = benchmark_comparison(small_arr, \"SMALL ARRAY\", iterations=100)\n",
    "speedup_medium = benchmark_comparison(medium_arr, \"MEDIUM ARRAY\", iterations=50)\n",
    "speedup_large = benchmark_comparison(large_arr, \"LARGE ARRAY\", iterations=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Small (1K):     {speedup_small:6.2f}x speedup\")\n",
    "print(f\"Medium (100K):  {speedup_medium:6.2f}x speedup\")\n",
    "print(f\"Large (1M):     {speedup_large:6.2f}x speedup\")\n",
    "print(\"\\n‚úÖ Key Insight: JIT speedup increases with array size!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING ASYNCHRONOUS EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîÑ UNDERSTANDING .block_until_ready()\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "JAX executes operations ASYNCHRONOUSLY by default for maximum performance.\n",
    "\n",
    "What this means:\n",
    "1. JAX queues operations and returns control to Python immediately\n",
    "2. Actual computation happens in the background on the accelerator\n",
    "3. Without .block_until_ready(), you'd measure queue time, not compute time!\n",
    "\n",
    "Example:\n",
    "    result = collatz(arr)           # Returns instantly, not done yet!\n",
    "    print(result)                   # NOW it blocks to get the value\n",
    "\n",
    "For accurate timing, ALWAYS use .block_until_ready() after the computation:\n",
    "    result = collatz(arr).block_until_ready()  # ‚úÖ Waits for completion\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING JAXPR - JAX's Intermediate Representation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç JAXPR - JAX's Intermediate Representation\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "JAXPR is like assembly language for JAX. It shows the low-level operations\n",
    "that XLA compiles into machine code. Think of it as a peek under the hood!\n",
    "\"\"\")\n",
    "\n",
    "sample_arr = jnp.arange(1, 11)\n",
    "print(f\"\\nJAXPR for collatz_with_jit with input shape {sample_arr.shape}:\")\n",
    "print(\"-\" * 70)\n",
    "print(jax.make_jaxpr(collatz_with_jit)(sample_arr))\n",
    "print()\n",
    "\n",
    "print(\"What you see:\")\n",
    "print(\"  ‚Ä¢ Input parameters (a:i32[10]) - integer array with 10 elements\")\n",
    "print(\"  ‚Ä¢ Primitive operations: mod, eq, where, floordiv, mul, add\")\n",
    "print(\"  ‚Ä¢ Data flow through the computation\")\n",
    "print(\"  ‚Ä¢ This gets sent to XLA for optimization and compilation!\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 2: Why Python if/else Fails with JIT\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"‚ö†Ô∏è  DEMONSTRATION: Why Python Control Flow Breaks JIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ‚ùå INCORRECT: Using Python if/else with array values\n",
    "@jax.jit\n",
    "def broken_conditional(x):\n",
    "    \"\"\"\n",
    "    This will behave INCORRECTLY when JIT-compiled!\n",
    "    During tracing, JAX doesn't know x's value, only its shape/type.\n",
    "    It picks ONE branch and always uses that branch.\n",
    "    \"\"\"\n",
    "    if x > 0:  # ‚ùå Compares abstract tracer, not actual value\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "print(\"\\n‚ùå Testing broken_conditional with Python if/else:\")\n",
    "try:\n",
    "    result_pos = broken_conditional(jnp.array(5.0))\n",
    "    print(f\"   broken_conditional(5.0)  = {result_pos}\")\n",
    "    \n",
    "    result_neg = broken_conditional(jnp.array(-5.0))\n",
    "    print(f\"   broken_conditional(-5.0) = {result_neg}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEM: Both give same result! Only one branch was compiled.\")\n",
    "    print(f\"   Expected: 10.0 and -15.0, but got {result_pos} and {result_neg}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "    print(f\"   This happens because JAX can't determine the branch during tracing!\")\n",
    "\n",
    "# ‚úÖ CORRECT: Using JAX-compatible control flow\n",
    "@jax.jit\n",
    "def correct_conditional(x):\n",
    "    \"\"\"\n",
    "    Correct version using jnp.where().\n",
    "    Evaluates BOTH branches and selects based on condition.\n",
    "    Works with JIT because no Python control flow is needed!\n",
    "    \"\"\"\n",
    "    return jnp.where(x > 0, x * 2, x * 3)\n",
    "\n",
    "print(\"\\n‚úÖ Testing correct_conditional with jnp.where():\")\n",
    "result_pos = correct_conditional(jnp.array(5.0))\n",
    "result_neg = correct_conditional(jnp.array(-5.0))\n",
    "print(f\"   correct_conditional(5.0)  = {result_pos}  ‚úì\")\n",
    "print(f\"   correct_conditional(-5.0) = {result_neg} ‚úì\")\n",
    "print(f\"   Both results are CORRECT!\")\n",
    "\n",
    "# Side-by-side comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: if/else vs jnp.where()\")\n",
    "print(\"=\" * 70)\n",
    "test_values = jnp.array([5.0, -3.0, 2.0, -8.0, 0.0])\n",
    "correct_results = correct_conditional(test_values)\n",
    "print(f\"Input values:     {test_values}\")\n",
    "print(f\"jnp.where() results: {correct_results}\")\n",
    "print(f\"Expected (x>0 ? 2x : 3x): [10.0, -9.0, 4.0, -24.0, 0.0]\")\n",
    "print(f\"Match: {jnp.allclose(correct_results, jnp.array([10.0, -9.0, 4.0, -24.0, 0.0]))}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 3: Side Effects in JIT - Print Statements\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üñ®Ô∏è  DEMONSTRATION: Side Effects Only Happen During Tracing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "@jax.jit\n",
    "def function_with_print(x):\n",
    "    \"\"\"\n",
    "    Print statements only execute during FIRST call (tracing phase).\n",
    "    Subsequent calls use cached compiled code without prints!\n",
    "    \"\"\"\n",
    "    print(f\"   üîç TRACING: Inside function with x = {x}\")\n",
    "    return x * 2\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  First call (triggers tracing and compilation):\")\n",
    "result1 = function_with_print(jnp.array(10.0))\n",
    "print(f\"   Result: {result1}\\n\")\n",
    "\n",
    "print(\"2Ô∏è‚É£  Second call (uses cached compiled version):\")\n",
    "result2 = function_with_print(jnp.array(20.0))\n",
    "print(f\"   Result: {result2}\")\n",
    "print(f\"   üëÜ Notice: The print INSIDE the function didn't execute!\\n\")\n",
    "\n",
    "print(\"3Ô∏è‚É£  Third call (still using cached version):\")\n",
    "result3 = function_with_print(jnp.array(30.0))\n",
    "print(f\"   Result: {result3}\")\n",
    "print(f\"   üëÜ Still no print - using cached compilation\\n\")\n",
    "\n",
    "print(\"üí° Key Point: Side effects (print, global vars, I/O) only happen once!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 4: When NOT to Use JIT - Compilation Overhead\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öñÔ∏è  DEMONSTRATION: JIT Overhead vs Benefit\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def simple_add_no_jit(x):\n",
    "    \"\"\"Simple addition without JIT.\"\"\"\n",
    "    return x + 1\n",
    "\n",
    "@jax.jit\n",
    "def simple_add_with_jit(x):\n",
    "    \"\"\"Simple addition with JIT.\"\"\"\n",
    "    return x + 1\n",
    "\n",
    "# SMALL COMPUTATION - JIT overhead dominates\n",
    "print(\"\\nüìâ TINY ARRAYS (3 elements, 1000 iterations):\")\n",
    "small_arr = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_no_jit(small_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "_ = simple_add_with_jit(small_arr).block_until_ready()  # Warm up\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_with_jit(small_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"  WITHOUT JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  WITH JIT:    {time_with_jit:.6f} seconds\")\n",
    "speedup = time_no_jit / time_with_jit\n",
    "print(f\"  Speedup:     {speedup:.2f}x\")\n",
    "if speedup < 1.5:\n",
    "    print(f\"  ‚ö†Ô∏è  JIT overhead isn't worth it for tiny computations!\")\n",
    "\n",
    "# LARGE COMPUTATION - JIT benefit is clear\n",
    "print(\"\\nüìà LARGE ARRAYS (1M elements, 100 iterations):\")\n",
    "large_arr = jnp.arange(1000000.0)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_no_jit(large_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "_ = simple_add_with_jit(large_arr).block_until_ready()  # Warm up\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_with_jit(large_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"  WITHOUT JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  WITH JIT:    {time_with_jit:.6f} seconds\")\n",
    "print(f\"  Speedup:     {time_no_jit/time_with_jit:.2f}x\")\n",
    "print(f\"  ‚úÖ JIT provides significant benefit for large computations!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ KEY TAKEAWAYS - JIT COMPILATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. ‚úÖ JIT provides 2x-100x speedup for large numerical computations\n",
    "2. ‚úÖ Use jnp.where() for conditionals, NOT Python if/else with array values\n",
    "3. ‚úÖ Pure functions work best (no side effects like print, globals, I/O)\n",
    "4. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
    "5. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
    "6. ‚úÖ JIT benefit increases with array size and computation complexity\n",
    "7. ‚úÖ Inspect with jax.make_jaxpr() to see the compiled representation\n",
    "8. ‚ùå Python control flow (if/else) that depends on array VALUES breaks JIT\n",
    "9. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
    "10. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
    "\n",
    "üí° Best Use Cases: Matrix operations, neural networks, scientific simulations,\n",
    "   repeated computations on large arrays\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc50953",
   "metadata": {},
   "source": [
    "## Quick Reference: JIT-Compatible Control Flow\n",
    "\n",
    "When you need conditionals in JIT-compiled functions, use these JAX operations:\n",
    "\n",
    "| Scenario | ‚ùå Don't Use | ‚úÖ Use Instead |\n",
    "|----------|--------------|----------------|\n",
    "| Element-wise conditional | `if x > 0: ...` | `jnp.where(x > 0, true_val, false_val)` |\n",
    "| Scalar conditional | `if x > 0: ...` | `jax.lax.cond(x > 0, true_fn, false_fn, operand)` |\n",
    "| Multiple branches | `if/elif/else` | `jax.lax.switch(index, branches, operand)` |\n",
    "| Dynamic loops | `for i in range(int(x)): ...` | `jax.lax.fori_loop(start, end, body_fn, init)` |\n",
    "| While loops | `while condition: ...` | `jax.lax.while_loop(cond_fn, body_fn, init)` |\n",
    "| Array updates | `arr[i] = val` | `arr.at[i].set(val)` |\n",
    "\n",
    "**Why?** During JIT tracing, JAX works with abstract values (shapes/types), not actual data. Python control flow needs concrete values, which aren't available during tracing. JAX's control flow ops are designed to work with abstract values!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
